"""VRA-78: Ant-ratio sweep v0 (probe cost + assoc capability + frontier plot).

This tool is an orchestrator. It produces runtime artifacts under repo-root:
  bench_vault/_tmp/vra78_ant_ratio_sweep_v0/<ts>/

Committed outputs are only code/tests/docs. The sweep artifacts are gitignored.

Important invariants:
- PASS/FAIL is always read from metrics.json/report.json, not exit codes.
- All run roots are created under repo-root bench_vault/_tmp/... by default.
"""

from __future__ import annotations

import argparse
import csv
import json
import os
import subprocess
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Tuple


class SweepError(RuntimeError):
    pass


def _repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def _now_ts() -> str:
    return time.strftime("%Y%m%d_%H%M%S", time.gmtime())


def _stable_json(obj: Any) -> str:
    return json.dumps(obj, sort_keys=True, indent=2, ensure_ascii=True) + "\n"


def _load_json(path: Path) -> dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def _tail_contains(path: Path, needle: str, max_bytes: int = 1_000_000) -> bool:
    if not path.exists():
        return False
    data: bytes
    with path.open("rb") as f:
        try:
            f.seek(0, os.SEEK_END)
            size = f.tell()
            if size > int(max_bytes):
                f.seek(size - int(max_bytes), os.SEEK_SET)
            else:
                f.seek(0, os.SEEK_SET)
            data = f.read()
        except OSError:
            data = f.read()
    return str(needle) in data.decode("utf-8", errors="replace")


def _is_probe_pass(metrics: Dict[str, Any]) -> bool:
    return bool(
        metrics.get("stability_pass") is True
        and metrics.get("had_oom") is False
        and metrics.get("had_nan") is False
        and metrics.get("had_inf") is False
    )


def _safe_tag(s: str) -> str:
    out = []
    for ch in str(s):
        if ch.isalnum() or ch in ("_", "-", "."):
            out.append(ch)
        else:
            out.append("_")
    return "".join(out)


def _ant_preset_for_tier(tier: str) -> str:
    t = str(tier).strip().lower()
    if t == "small":
        return "OD1_CANON_SMALL"
    if t == "real":
        return "OD1_CANON_REAL"
    if t == "stress":
        return "OD1_CANON_STRESS"
    raise SweepError(f"unknown ant_tier: {tier!r} (expected small,real,stress)")


def _ant_shape_for_tier(tier: str) -> Tuple[int, int]:
    t = str(tier).strip().lower()
    if t == "small":
        return 2048, 256
    if t == "real":
        return 8192, 576
    if t == "stress":
        return 16384, 768
    raise SweepError(f"unknown ant_tier: {tier!r} (expected small,real,stress)")


@dataclass(frozen=True)
class TokenBudget:
    token_budget: int
    min_steps: int
    max_steps: int
    seq_len: int

    def steps_for_batch(self, batch: int) -> int:
        tokens_per_step = int(batch) * int(self.seq_len)
        if tokens_per_step <= 0:
            return int(self.min_steps)
        raw = int(self.token_budget) // int(tokens_per_step)
        return max(int(self.min_steps), min(int(self.max_steps), int(raw)))


def _run_probe(
    *,
    repo_root: Path,
    out_dir: Path,
    ant: str,
    colony: str,
    out_dim: int,
    batch: int,
    warmup_steps: int,
    measure_steps: int,
    precision: str,
    amp: int,
    force_device: str,
    timeout_s: int,
) -> Tuple[Path, Dict[str, Any]]:
    tool = repo_root / "Golden Draft" / "tools" / "gpu_capacity_probe.py"
    if not tool.exists():
        raise SweepError(f"probe harness missing: {tool}")

    out_dir.mkdir(parents=True, exist_ok=True)
    cmd = [
        sys.executable,
        str(tool),
        "--ant",
        str(ant),
        "--colony",
        str(colony),
        "--out-dim",
        str(int(out_dim)),
        "--batch",
        str(int(batch)),
        "--warmup-steps",
        str(int(warmup_steps)),
        "--measure-steps",
        str(int(measure_steps)),
        "--precision",
        str(precision),
        "--amp",
        str(int(amp)),
        "--output-dir",
        str(out_dir),
    ]
    env = dict(os.environ)
    if force_device:
        env["VRX_FORCE_DEVICE"] = str(force_device)

    try:
        cp = subprocess.run(cmd, cwd=str(repo_root), env=env, timeout=max(1, int(timeout_s)))
        rc = int(cp.returncode)
    except subprocess.TimeoutExpired as exc:
        raise SweepError(f"probe timeout after {int(timeout_s)}s: out_dir={out_dir}") from exc
    if int(rc) == 2:
        raise SweepError(f"probe harness rc=2 (invalid args or cannot write): out_dir={out_dir}")

    metrics_path = out_dir / "metrics.json"
    if not metrics_path.exists():
        raise SweepError(f"probe metrics missing: {metrics_path}")
    metrics = _load_json(metrics_path)
    return out_dir, metrics


def _run_capability_train(
    *,
    repo_root: Path,
    run_root: Path,
    seed: int,
    device: str,
    precision: str,
    ring_len: int,
    slot_dim: int,
    expert_heads: int,
    batch: int,
    steps: int,
    synth_len: int,
    assoc_keys: int,
    assoc_pairs: int,
    assoc_val_range: int,
    max_samples: int,
    eval_samples: int,
    ptr_dtype: str,
    offline_only: bool,
    save_every: int,
    timeout_s: int,
) -> Tuple[Path, int]:
    """Run one synth assoc_byte training job and return checkpoint path."""

    run_root.mkdir(parents=True, exist_ok=True)
    ckpt_path = run_root / "checkpoint.pt"
    log_path = run_root / "vraxion.log"

    env = dict(os.environ)
    env.update(
        {
            "VAR_PROJECT_ROOT": str(run_root),
            "VAR_LOGGING_PATH": str(log_path),
            "VAR_COMPUTE_DEVICE": str(device),
            "VRX_MODE": "train",
            "VRX_PRECISION": str(precision),
            "VRX_PTR_DTYPE": str(ptr_dtype),
            "VAR_RUN_SEED": str(int(seed)),
            "VRX_OFFLINE_ONLY": "1" if offline_only else "0",
            # Shape
            "VRX_RING_LEN": str(int(ring_len)),
            "VRX_SLOT_DIM": str(int(slot_dim)),
            "VRX_EXPERT_HEADS": str(int(expert_heads)),
            # Synth assoc_byte
            "VRX_SYNTH": "1",
            "VRX_SYNTH_ONCE": "1",
            "VRX_SYNTH_MODE": "assoc_byte",
            "VRX_SYNTH_LEN": str(int(synth_len)),
            "VRX_ASSOC_KEYS": str(int(assoc_keys)),
            "VRX_ASSOC_PAIRS": str(int(assoc_pairs)),
            "VRX_ASSOC_VAL_RANGE": str(int(assoc_val_range)),
            # Loop
            "VRX_BATCH_SIZE": str(int(batch)),
            "VRX_MAX_SAMPLES": str(int(max_samples)),
            "VRX_EVAL_SAMPLES": str(int(eval_samples)),
            "VRX_MAX_STEPS": str(int(steps)),
            # Ensure bounded sweep runs cannot be overridden by ambient shell env.
            "VRX_IGNORE_MAX_STEPS": "0",
            # The modern runner path is phase-driven; set both phase and max caps
            # so capability loops remain bounded and deterministic.
            "VRX_PHASE_A_STEPS": str(int(steps)),
            "VRX_PHASE_B_STEPS": "0",
            # Sweep runs only need checkpoints; disable in-loop checkpoint eval to
            # avoid device-mismatch failures in eval paths and keep run cost stable.
            "VRX_EVAL_AT_CHECKPOINT": "0",
            "VRX_EVAL_EVERY_STEPS": "0",
            # Saving
            "VRX_RESUME": "0",
            "VRX_CKPT": str(ckpt_path),
            "VRX_SAVE_LAST_GOOD": "1",
            # Keep both names: modern runner consumes SAVE_EVERY_STEPS, while
            # older paths may still inspect SAVE_EVERY.
            "VRX_SAVE_EVERY_STEPS": str(int(save_every)),
            "VRX_SAVE_EVERY": str(int(save_every)),
            "VRX_SAVE_HISTORY": "0",
        }
    )

    cmd = [sys.executable, "-u", str(repo_root / "Golden Draft" / "vraxion_run.py")]
    def _first_checkpoint() -> Optional[Path]:
        if ckpt_path.exists():
            return ckpt_path
        for name in ("checkpoint_last_good.pt", "checkpoint.pt"):
            cand = run_root / name
            if cand.exists():
                return cand
        return None

    try:
        cp = subprocess.run(cmd, cwd=str(repo_root), env=env, timeout=max(1, int(timeout_s)))
        rc = int(cp.returncode)
    except subprocess.TimeoutExpired as exc:
        recovered = _first_checkpoint()
        # Artifact-truth: if timeout happens after checkpoint materialization,
        # continue and surface timeout via nonzero rc for ranking/debug.
        if recovered is not None:
            return recovered, 124
        raise SweepError(f"capability train timeout after {int(timeout_s)}s (run_root={run_root})") from exc

    # Artifact-truth: treat an existing checkpoint as success even if the
    # subprocess return code is nonzero (some Windows paths surface -1 / 255
    # despite writing a valid checkpoint).
    recovered = _first_checkpoint()
    if recovered is not None:
        return recovered, rc

    # No checkpoint means we cannot proceed, regardless of rc.
    raise SweepError(f"checkpoint not found under run_root={run_root} (rc={rc})")


def _run_capability_eval(
    *,
    repo_root: Path,
    run_root: Path,
    checkpoint: Path,
    eval_samples: int,
    batch_size: int,
    device: str,
    eval_seed_offset: int,
    force_disjoint: bool,
    timeout_s: int,
    heartbeat_s: int,
) -> Tuple[Path, bool]:
    tool = repo_root / "Golden Draft" / "tools" / "eval_ckpt_assoc_byte.py"
    if not tool.exists():
        raise SweepError(f"capability eval tool missing: {tool}")

    cmd: List[str] = [
        sys.executable,
        str(tool),
        "--run-root",
        str(run_root),
        "--checkpoint",
        str(checkpoint),
        "--eval-samples",
        str(int(eval_samples)),
        "--batch-size",
        str(int(batch_size)),
        "--device",
        str(device),
        "--eval-seed-offset",
        str(int(eval_seed_offset)),
        "--heartbeat-s",
        str(max(1, int(heartbeat_s))),
    ]
    if force_disjoint:
        cmd.append("--force-eval-disjoint")

    try:
        timeout: Optional[int]
        timeout = None if int(timeout_s) <= 0 else max(1, int(timeout_s))
        cp = subprocess.run(cmd, cwd=str(repo_root), timeout=timeout)
        rc = int(cp.returncode)
    except subprocess.TimeoutExpired as exc:
        raise SweepError(f"capability eval timeout after {int(timeout_s)}s (run_root={run_root})") from exc

    rep = run_root / "report.json"
    eval_log = run_root / "vraxion_eval.log"
    heartbeat_seen = _tail_contains(eval_log, "[eval_ckpt][heartbeat]")
    # Artifact-truth: if report.json exists, keep it even if the subprocess rc is nonzero.
    if rep.exists():
        return rep, heartbeat_seen
    if int(rc) != 0:
        raise SweepError(f"capability eval rc={rc} (run_root={run_root})")
    if not rep.exists():
        raise SweepError(f"missing report.json after eval: {rep}")
    return rep, heartbeat_seen


def _write_csv(path: Path, rows: List[Dict[str, Any]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    if not rows:
        path.write_text("", encoding="utf-8")
        return
    keys = list(rows[0].keys())
    with path.open("w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=keys)
        w.writeheader()
        for r in rows:
            w.writerow({k: r.get(k) for k in keys})


def _parse_args(argv: Optional[Sequence[str]]) -> argparse.Namespace:
    ap = argparse.ArgumentParser(description="Run ant-ratio sweep v0 (VRA-78).")
    ap.add_argument("--batch-targets", required=True, help="Path to ant_ratio_batch_targets_v0.json")
    ap.add_argument("--out-root", default="", help="Default: bench_vault/_tmp/vra78_ant_ratio_sweep_v0/<ts>/")
    # Probe track
    ap.add_argument("--probe-colony", default="OD1_CANON_REAL")
    ap.add_argument("--probe-precision", default="fp16")
    ap.add_argument("--probe-amp", type=int, default=1)
    ap.add_argument("--probe-warmup-steps", type=int, default=5)
    ap.add_argument("--probe-measure-steps", type=int, default=50)
    ap.add_argument("--probe-force-device", default="", help="Optional VRX_FORCE_DEVICE override passed to probe.")
    # Capability track
    ap.add_argument("--cap-device", default="cuda", choices=["cpu", "cuda"])
    ap.add_argument(
        "--cap-eval-device",
        default=None,
        choices=["cpu", "cuda"],
        help="Device for capability eval (default: cap-device).",
    )
    ap.add_argument(
        "--cap-eval-fallback-device",
        default=None,
        choices=["cpu", "cuda"],
        help="Optional fallback device for capability eval if the first attempt fails.",
    )
    ap.add_argument("--cap-precision", default="fp32")
    ap.add_argument("--cap-seed", type=int, default=123)
    ap.add_argument("--cap-ptr-dtype", default="fp64")
    ap.add_argument("--cap-offline-only", action="store_true", default=True)
    ap.add_argument("--cap-save-every", type=int, default=50)
    ap.add_argument("--cap-eval-samples", type=int, default=4096)
    ap.add_argument("--cap-eval-seed-offset", type=int, default=1000003)
    ap.add_argument("--cap-force-disjoint", action="store_true", default=True)
    ap.add_argument("--cap-synth-len", type=int, default=256)
    ap.add_argument("--cap-assoc-keys", type=int, default=64)
    ap.add_argument("--cap-assoc-pairs", type=int, default=4)
    ap.add_argument("--cap-assoc-val-range", type=int, default=256)
    ap.add_argument("--cap-max-samples", type=int, default=8192)
    ap.add_argument("--probe-timeout-s", type=int, default=900)
    ap.add_argument("--cap-train-timeout-s", type=int, default=900)
    ap.add_argument("--cap-eval-timeout-s", type=int, default=900)
    ap.add_argument("--cap-eval-heartbeat-s", type=int, default=60)
    ap.add_argument("--cap-eval-retry-once", type=int, default=1, choices=[0, 1])
    ap.add_argument("--flush-every-config", type=int, default=1, choices=[0, 1])
    # Fairness
    ap.add_argument("--seq-len", type=int, default=256, help="Accounting seq_len used for token budget computation.")
    ap.add_argument("--token-budget", type=int, default=1_000_000, help="Fixed token budget for capability runs.")
    ap.add_argument("--min-steps", type=int, default=50)
    ap.add_argument("--max-steps", type=int, default=2000)
    return ap.parse_args(list(argv) if argv is not None else None)


def main(argv: Optional[Sequence[str]] = None) -> int:
    args = _parse_args(argv)
    repo_root = _repo_root()

    out_root = Path(args.out_root).resolve() if str(args.out_root).strip() else (repo_root / "bench_vault" / "_tmp" / "vra78_ant_ratio_sweep_v0" / _now_ts())
    out_root.mkdir(parents=True, exist_ok=True)

    targets = _load_json(Path(args.batch_targets).resolve())
    rows = targets.get("rows") or []
    if not isinstance(rows, list):
        raise SystemExit("batch-targets JSON missing rows[]")

    # These live next to this script under Golden Draft/tools/.
    from ant_ratio_packet_v0 import TokenBudget as PacketBudget, build_packet  # type: ignore
    from ant_ratio_plot_v0 import build_html  # type: ignore

    tb = TokenBudget(
        token_budget=int(args.token_budget),
        min_steps=int(args.min_steps),
        max_steps=int(args.max_steps),
        seq_len=int(args.seq_len),
    )
    pkt_budget = PacketBudget(token_budget=int(args.token_budget), min_steps=int(args.min_steps), max_steps=int(args.max_steps))

    packets_jsonl = out_root / "ant_ratio_packets.jsonl"
    summary_csv = out_root / "ant_ratio_summary.csv"
    html_out = out_root / "ant_ratio_frontier_v0.html"

    csv_rows: List[Dict[str, Any]] = []

    failures: List[Dict[str, Any]] = []

    cap_eval_device = str(args.cap_eval_device or args.cap_device)
    cap_eval_fallback_device = str(args.cap_eval_fallback_device) if args.cap_eval_fallback_device else ""
    if cap_eval_fallback_device and cap_eval_fallback_device == cap_eval_device:
        cap_eval_fallback_device = ""

    def _flush_outputs() -> None:
        _write_csv(summary_csv, csv_rows)

        packets: List[Dict[str, Any]] = []
        if packets_jsonl.exists():
            for ln in packets_jsonl.read_text(encoding="utf-8", errors="replace").splitlines():
                s = ln.strip()
                if not s:
                    continue
                packets.append(json.loads(s))

        html = build_html(packets=packets, title="VRAXION Ant Ratio Frontier v0")
        html_out.write_text(html, encoding="utf-8")

        meta = {
            "schema_version": "ant_ratio_sweep_v0",
            "generated_utc": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "out_root": str(out_root),
            "batch_targets": str(Path(args.batch_targets).resolve()),
            "token_budget": int(args.token_budget),
            "seq_len": int(args.seq_len),
            "min_steps": int(args.min_steps),
            "max_steps": int(args.max_steps),
            "cap_device": str(args.cap_device),
            "cap_eval_device": str(cap_eval_device),
            "cap_eval_fallback_device": str(cap_eval_fallback_device),
            "heartbeat_s": int(args.cap_eval_heartbeat_s),
            "eval_timeout_mode": "disabled" if int(args.cap_eval_timeout_s) <= 0 else "fixed",
            "flush_every_config": bool(int(args.flush_every_config)),
        }
        (out_root / "sweep_meta.json").write_text(_stable_json(meta), encoding="utf-8")
        (out_root / "sweep_failures.json").write_text(_stable_json({"failures": failures}), encoding="utf-8")

    for r in rows:
        if not isinstance(r, dict):
            continue
        if bool(r.get("unusable")):
            continue
        tier = str(r.get("ant_tier") or "").strip().lower()
        eh = r.get("expert_heads")
        batch = r.get("chosen_batch")
        if tier not in ("small", "real", "stress"):
            continue
        if not isinstance(eh, int) or not isinstance(batch, int):
            continue

        cfg_tag = _safe_tag(f"{tier}_E{eh}_B{batch:04d}")

        try:
            # 1) Probe run (cost metrics)
            probe_dir = out_root / "runs_probe" / cfg_tag
            probe_dir.mkdir(parents=True, exist_ok=True)
            probe_out_dir = probe_dir / "probe"
            probe_out, probe_metrics = _run_probe(
                repo_root=repo_root,
                out_dir=probe_out_dir,
                ant=_ant_preset_for_tier(tier),
                colony=str(args.probe_colony),
                out_dim=int(eh),
                batch=int(batch),
                warmup_steps=int(args.probe_warmup_steps),
                measure_steps=int(args.probe_measure_steps),
                precision=str(args.probe_precision),
                amp=int(args.probe_amp),
                force_device=str(args.probe_force_device).strip(),
                timeout_s=int(args.probe_timeout_s),
            )

            # 2) Capability run (train + postmortem eval)
            ring_len, slot_dim = _ant_shape_for_tier(tier)
            steps = tb.steps_for_batch(int(batch))
            assoc_dir = out_root / "runs_assoc" / cfg_tag / f"seed{int(args.cap_seed)}"
            # Capability runs in this sweep are intentionally short/bounded. The
            # wallclock trainer checks MAX_STEPS before checkpoint cadence, so a
            # larger save interval can miss all saves. Force every-step saving to
            # guarantee checkpoint availability for postmortem eval.
            save_every = 1
            ckpt, cap_train_rc = _run_capability_train(
                repo_root=repo_root,
                run_root=assoc_dir,
                seed=int(args.cap_seed),
                device=str(args.cap_device),
                precision=str(args.cap_precision),
                ring_len=int(ring_len),
                slot_dim=int(slot_dim),
                expert_heads=int(eh),
                batch=int(batch),
                steps=int(steps),
                synth_len=int(args.cap_synth_len),
                assoc_keys=int(args.cap_assoc_keys),
                assoc_pairs=int(args.cap_assoc_pairs),
                assoc_val_range=int(args.cap_assoc_val_range),
                max_samples=int(args.cap_max_samples),
                eval_samples=int(args.cap_eval_samples),
                ptr_dtype=str(args.cap_ptr_dtype),
                offline_only=bool(args.cap_offline_only),
                save_every=int(save_every),
                timeout_s=int(args.cap_train_timeout_s),
            )
            used_eval_device = cap_eval_device
            eval_heartbeat_seen = False
            attempt_eval_count = 0

            def _run_eval_once(device_name: str) -> None:
                nonlocal eval_heartbeat_seen, attempt_eval_count
                attempt_eval_count += 1
                _, hb_seen = _run_capability_eval(
                    repo_root=repo_root,
                    run_root=assoc_dir,
                    checkpoint=ckpt,
                    eval_samples=int(args.cap_eval_samples),
                    batch_size=int(batch),
                    device=str(device_name),
                    eval_seed_offset=int(args.cap_eval_seed_offset),
                    force_disjoint=bool(args.cap_force_disjoint),
                    timeout_s=int(args.cap_eval_timeout_s),
                    heartbeat_s=int(args.cap_eval_heartbeat_s),
                )
                eval_heartbeat_seen = bool(eval_heartbeat_seen or hb_seen)

            eval_exc: Optional[Exception] = None
            eval_done = False
            for dev_name in [cap_eval_device] + ([cap_eval_fallback_device] if cap_eval_fallback_device else []):
                used_eval_device = str(dev_name)
                try:
                    _run_eval_once(str(dev_name))
                    eval_done = True
                    break
                except Exception as exc:
                    eval_exc = exc
                    may_retry = bool(int(args.cap_eval_retry_once)) and assoc_dir.exists() and "missing report.json" in str(exc)
                    if may_retry:
                        try:
                            _run_eval_once(str(dev_name))
                            eval_done = True
                            break
                        except Exception as retry_exc:
                            eval_exc = retry_exc
            if not eval_done:
                if eval_exc is not None:
                    raise eval_exc
                raise SweepError(f"capability eval failed without exception (run_root={assoc_dir})")

            # 3) Join into packet and append.
            pkt = build_packet(
                probe_run_root=probe_out,
                assoc_run_root=assoc_dir,
                ant_tier_override=str(tier),
                token_budget=pkt_budget,
                capability_steps_override=int(steps),
            )
            pkt["cap_train_rc"] = int(cap_train_rc)
            pkt["cap_train_nonzero_rc"] = bool(int(cap_train_rc) != 0)
            pkt["cap_eval_device"] = str(used_eval_device)
            with packets_jsonl.open("a", encoding="utf-8") as f:
                f.write(json.dumps(pkt, sort_keys=True, ensure_ascii=True) + "\n")

            vram_ratio = pkt.get("vram_ratio_reserved")
            throughput_tokens = pkt.get("throughput_tokens_per_s")
            capability_acc = pkt.get("assoc_byte_disjoint_accuracy")
            cap_per_token: Optional[float] = None
            cap_per_vram: Optional[float] = None
            tokens_per_vram: Optional[float] = None
            target_abs_error: Optional[float] = None
            if isinstance(vram_ratio, (int, float)) and float(vram_ratio) > 0.0:
                if isinstance(throughput_tokens, (int, float)) and float(throughput_tokens) > 0.0:
                    tokens_per_vram = float(throughput_tokens) / float(vram_ratio)
                if isinstance(capability_acc, (int, float)):
                    cap_per_vram = float(capability_acc) / float(vram_ratio)
                    target_abs_error = abs(float(vram_ratio) - 0.85)
            if isinstance(capability_acc, (int, float)) and isinstance(throughput_tokens, (int, float)) and float(throughput_tokens) > 0.0:
                cap_per_token = float(capability_acc) / float(throughput_tokens)

            csv_rows.append(
                {
                    "status": "ok",
                    "error": "",
                    "ant_tier": tier,
                    "ant_body_cells": pkt.get("ant_body_cells"),
                    "ant_body_scale_vs_small": pkt.get("ant_body_scale_vs_small"),
                    "expert_heads": int(eh),
                    "batch": int(batch),
                    "steps": int(steps),
                    "cap_train_rc": int(cap_train_rc),
                    "cap_train_nonzero_rc": bool(int(cap_train_rc) != 0),
                    "cap_eval_device": str(used_eval_device),
                    "eval_heartbeat_seen": bool(eval_heartbeat_seen),
                    "attempt_eval_count": int(attempt_eval_count),
                    "probe_pass": bool(_is_probe_pass(probe_metrics)),
                    "vram_ratio_reserved": vram_ratio,
                    "vram_target_abs_error": target_abs_error,
                    "throughput_tokens_per_s": throughput_tokens,
                    "tokens_per_vram_ratio": tokens_per_vram,
                    "assoc_byte_disjoint_accuracy": capability_acc,
                    "assoc_acc_per_token_per_s": cap_per_token,
                    "assoc_acc_per_vram_ratio": cap_per_vram,
                    "probe_run_root": pkt.get("probe_run_root"),
                    "assoc_run_root": pkt.get("assoc_run_root"),
                }
            )
        except Exception as exc:
            err = f"{type(exc).__name__}: {exc}"
            failures.append({"config": cfg_tag, "error": err})
            print(f"[vra78][warn] {cfg_tag}: {err}")
            csv_rows.append(
                {
                    "status": "error",
                    "error": err,
                    "ant_tier": tier,
                    "ant_body_cells": "",
                    "ant_body_scale_vs_small": "",
                    "expert_heads": int(eh),
                    "batch": int(batch),
                    "steps": "",
                    "cap_train_rc": "",
                    "cap_train_nonzero_rc": "",
                    "cap_eval_device": "",
                    "eval_heartbeat_seen": "",
                    "attempt_eval_count": "",
                    "probe_pass": "",
                    "vram_ratio_reserved": "",
                    "vram_target_abs_error": "",
                    "throughput_tokens_per_s": "",
                    "tokens_per_vram_ratio": "",
                    "assoc_byte_disjoint_accuracy": "",
                    "assoc_acc_per_token_per_s": "",
                    "assoc_acc_per_vram_ratio": "",
                    "probe_run_root": "",
                    "assoc_run_root": "",
                }
            )
        finally:
            # Make partial results durable (useful if the sweep is interrupted).
            if bool(int(args.flush_every_config)):
                try:
                    _flush_outputs()
                except Exception as exc:
                    print(f"[vra78][warn] flush failed: {type(exc).__name__}: {exc}")

    # Final flush + friendly output pointers.
    _flush_outputs()

    print(f"[vra78] packets: {packets_jsonl}")
    print(f"[vra78] csv: {summary_csv}")
    print(f"[vra78] html: {html_out}")
    if failures:
        print(f"[vra78] failures: {len(failures)} (see sweep_failures.json)")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
